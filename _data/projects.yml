- id: hammer
  title: "HAMMER"
  longtitle: "Hybridization of model-based and data-driven methods for robot locomotion and mobility"
  pi: carong
  year-start: 2025
  year-end: 2031
  funding: France 2030 - PEPR Robotics
  active: true
  partners: [cnrs,inria,onera,mines]
  description: "
<p>HAMMER is the Target Project 2 of the Prioritary Program of Research and Equipment of Acceleration in Robotics funded by the France 2030 program managed by ANR (Agence Nationale de la Recherche / French national research agency). In this large collaborative project led by <a href=https://www.i3s.unice.fr/en/>CNRS-U Nice I3S</a>, CNRS-AIST JRL is a partner with <a href=https://icb.cnrs.fr/>CNRS-UBE ICB</a>, <a href=https://www.onera.fr/en/tis>ONERA DITIS</a>, <a href=https://www.gipsa-lab.grenoble-inp.fr/en>CNRS-INP GIPSA-lab</a>, <a href=https://www.isir.upmc.fr/?lang=en>CNRS-Sorbonne Univ. ISIR</a>, <a href=https://www.laas.fr/en/teams/gepetto/>LAAS-CNRS</a>, <a href=https://www.inria.fr/en/willow>Inria-ENS Willow</a>, <a href=https://team.inria.fr/acentauri>Inria ACENTAURI</a>, <a href=https://team.inria.fr/larsen/>Inria Larsen</a> and <a href=https://www.minesparis.psl.eu/en/home/>Mines Paris PSL</a>. The aim is to endow robots with advanced locomotion and mobility capabilities, by developing a hybrid approach combining advantages of model-based and data-driven methods. </p>
<p>More specifically to JRL, we will focus on combining learning-based with model-based control to ensure strong control guarantees with LAAS-CNRS and also combining learning-based with model-based vision for improving prediction capabilities with Inria ACENTAURI. 
</p>
"
- id: calcadn
  title: "CalcADN"
  longtitle: "CalcADN: Computing on DNA"
  pi: carong
  year-start: 2025
  year-end: 2027
  funding: France 2030
  active: true
  partners: [cnrs,univevry,univorleans]
  description: "
<p>CalcADN is a collaborative project funded by the France 2030 program managed by ANR (Agence Nationale de la Recherche / French national research agency). In this high risk research project led by both <a href=https://limms-tokyo.org/>CNRS-U Tokyo LIMMS</a> and <a href=https://www.ens-lyon.fr/LIP/>CNRS-ENS Lyon LIP</a>, CNRS-AIST JRL is a partner with <a href=https://www.ibisc.univ-evry.fr/en/>U Evry IBISC</a>, <a href=https://www.univ-orleans.fr/lifo/>U Orleans LIFO</a>, <a href=https://lasir.cnrs.fr/en/lasir/>CNRS-UdL LASIRe</a>, <a href=https://www.gulliver.espci.fr/?-home->CNRS-ESPCI Gulliver</a> and <a href=https://www.iemn.fr/en/>CNRS-UdL IEMN</a>. The aim is to create the first computer that makes large scale computations on DNA by combining research on molecular computation, computer science and robotics for which JRL contributes in order to mimic expert bilogists with robots in manipulating liquids accurately at large scale with a high degree of repeatability. </p>
<p>CalcADN will be implemented on dual-arm robot.  
</p>
"
- id: foundation
  title: "RoboticsFoundationModel"
  longtitle: "Research and Development on Generative AI Foundation Model in Physical Domain"
  longtitle-jp: "フィジカル領域の生成AI基盤モデルに関する研究開発"
  pi: murooka
  year-start: 2024.4
  year-end: 2027.3
  funding: AIST
  active: true
  description: >
    <p>We aim to develop and integrate AI foundation models of various modalities such as language, images, acoustics, and 3D point clouds to build robotics foundation models that enables motion planning and control of robots in the real world. The CNRS-AIST JRL, in collaboration with the Industrial CPS Research Center and the Artificial Intelligence Research Center, is working to develop robotics foundation models through the development of imitation learning methods that can realize difficult manipulation tasks in a data-driven manner.</p>
  description-jp: >
    <p>言語・画像・音響・三次元点群等の多様なモダリティのAI基盤モデルを開発し統合することでロボットの実世界における動作計画・制御を可能とするロボット基盤モデルを構築することを目指す。AIST-CNRSロボット工学連携研究ラボでは、インダストリアルCPS研究センターや人工知能研究センターと協力しながら、センサ計測に応じた繊細なロボットの運動が必要となる難易度の高いマニピュレーションタスクをデータドリブンに実現可能な模倣学習の開発を通してロボット基盤モデルの構築に取り組んでいる。</p>
- id: kakenhi_wakate2022_murooka
  title: "InteractivePlanning"
  longtitle: "Interactive Robot Motion Planning System Capable of Acquiring Information by Human Instruction"
  longtitle-jp: "人間からの教示により情報獲得可能な対話型ロボット動作計画システム"
  pi: murooka
  year-start: 2022.4
  year-end: 2026.3
  funding: JSPS Grant-in-Aid for Early-Career Scientists
  funding-jp: 日本学術振興会 科学研究費助成事業（科研費） 若手研究
  active: true
  description: >
    <p>We aim to build a robotic system that enables humanoid robots and mobile manipulators to perform tasks that are difficult to execute fully autonomously with conventional planning methods by making complementary use of human teaching through demonstrations, gestures, and speech. In planning methods based on search and optimization, on-site human teaching of task-specific unknown information or information that requires exploration time enables the robot to immediately adapt to a new task and realize its behavior without the need for software modification.</p>
  description-jp: >
    <p>人型ロボットや移動型マニピュレータが、複雑環境三次元移動、大型物体運搬、複雑組み立て作業などの従来の計画手法では完全自律実行の困難であったタスクを、人間による実演・身振り・発話による教示を補完的に活用することで実現するためのロボットシステムを構築する。探索・最適化に基づいた計画手法において、タスク固有の未知情報や探索に時間を要する情報を人間がオンサイトで教示することで、ソフトウェアの変更を必要とせずに、ロボットが新しいタスクに即座に適応し動作を実現することを可能とする。</p>
- id: call
  title: "CALL"
  longtitle: "CALL: Cobotic AppLication for handover and Load transport"
  pi: carong
  year-start: 2025
  year-end: 2028
  funding: ANR
  active: true
  partners: [cnrs,tlse3,enit,uds]
  description: "
<p>CALL is a collaborative project funded by ANR (Agence Nationale de la Recherche / French national research agency). In this project led by <a href=https://www.laas.fr/en/>CNRS LAAS</a>, CNRS-AIST JRL is a partner with <a href=https://www.lgp.enit.fr/en/index.html>ENIT LGP</a> and <a href=https://www.usherbrooke.ca/3it/en/>UDS 3IT</a>. The aim is to combine the biomechanical modeling of humans with full-spherical visual perception and optimal robot control to enable the dynamic object handover between a human and a robot. </p>
<p>CALL will be implemented on robot arm and mobile manipulator.  
</p>
"
- id: antnoid
  title: "Ant'noid"
  longtitle: "Ant'noid: ant visual memory for humanoid robot autonomous navigation"
  pi: carong
  year-start: 2024
  year-end: 2028
  funding: ANR
  active: true
  partners: [cnrs,amu]
  description: "
<p>Ant'noid is a collaborative project funded by ANR (Agence Nationale de la Recherche / French national research agency). In this project led by CNRS-AIST JRL, we've partnered with CNRS-Aix-Marseille University <a href=https://ism.univ-amu.fr/en>Institute of Movement Science</a>. The aim is to leverage the knowledge on desert ants' visual familiarity capabilities to make a humanoid robot navigate both in bidimensional and tridimensional spaces with an extremely low memory usage from full-spherical visual perception. This project emerges from an International Emerging Action funded by CNRS.</p>
<p>Ant'noid will be implemented on small size and human adult size humanoids such as HRP-5P. 
</p>
"
- id: m2sv
  title: "M2SV"
  longtitle: "M2SV: Mobile Manipulator control based on Spherical Vision"
  pi: carong
  year-start: 2023
  year-end: 2025
  funding: CNRS GDR ISIS
  active: true
  partners: [utbm,upjv]
  description: "
<p>M2SV is an exploratory project funded by CNRS&rsquo; Special Interest Group on Information, Signal, Image and Vision (GDR ISIS). In this project led by UTBM, CIAD lab, CNRS-AIST JRL is a partner as UPJV, MIS lab (France) too. The aim is to enable the direct visual servoing of both the mobile base and the robot arm with a single spherical camera onboard the mobile base. Thus, a combined eye-in-hand / eye-t-hand visual servo is to be considered. This project emerges from <a href=https://unit.aist.go.jp/isri/isri-jrl/en/projects/project-dvsstraight.html target=_blank>DVS-straight</a>.</p>
    <h4>The goals are:</h4>
    <ul>
      <li>Optimal placement of the spherical camera</li>
      <li>Eye-in-hand spherical vision-based control of the robot base</li>
			<li>Eye-to-hand spherical vision-based control of the robot arm</li>
    </ul>
<p>M2SV system will be integrated on mobile manipulators of UTBM and JRL and assessed in experiments. 
</p>
"
- id: zero
  title: "Z"
  longtitle: "Z: Zero-Shot Learning of Real-World AI Connecting Large-Scale Learning Models and 3D Virtual Space"
  pi: yoshiyasu
  year-start: 2023
  year-end: 2026
  funding: JSPS
  active: true
  partners: [ttech]
  description: "
<p>This project, led by AIST's AIRC whose CNRS-AIST JRL is partner together with Tokyo Tech aims at leveraging the large learning models for 3D human learning, High-level object recognition and robot exploration, learned feature maps-based visual servoing.</p>
"
- id: janus
  title: "Janus"
  longtitle: "Team Janus - ANA Avatar XPRIZE"
  year-start: 2018
  year-end: 2022
  active: false
  url: "./janus/team-janus.html"
  partners: [cnrs,lirmm,doublegiken]
  description: "
<p>In Roman mythology, Janus represents the transition from the past to the future, like the one we are seeing today with new technologies. It also represents bridges and connections, like the ones we are building between humans and robots: two entities into the same body.</p>

<p>JANUS is a bi-located team that gathers expertise from the Japanese National Institute of Advanced Industrial Science and Technology (AIST) and the French National Centre for Scientific Research CNRS (CNRS), namely CNRS-AIST Joint Robotics Laboratory (JRL) and <a href=https://www.lirmm.fr/>LIRMM</a>, <a href=https://www.lirmm.fr/lirmm_eng/research/teams/idh>Interactive Digital Human</a> laboratory in Montpellier. These teams have extensive experience in humanoid robotics, haptics, brain computer interfaces, control, telerobotics and telepresence, embodiment and physical avatars. AIST is the co-developer with Kawada industries of the world renown HRP humanoid families and the main developer of the HRP Miim, the first effectively walking geminoid called cybernetic human and has extensive expertise in humanoid robotics. CNRS posses the only HRP-4 humanoid present in Europe. CNRS and AIST to some extent, were also partners of the EU Virtual Embodiment and Robotic Re-Embodiment (VERE) project where a telepresence embodiment station featuring advanced multi-modal feedback decodes the participant’s motor intentions at the EEG brain level to control humanoid robots even over large distances. CNRS with AIST joint efforts in the last EU Comanoid project where humanoid robots are envisioned to work in large-scale aeronautics manufacturing.</p>

<b>Why are we competing in ANA Avatar XPRIZE?</b>

<p>Our main mission is to create and advance knowledge for the well-being of humanity and society. Challenging competitions like ANA Avatar XPRIZE are the technological catalyzers for new ideas and technologies. Furthermore, the Covid19 outbreak highlighted the need for such an avatar technology in healthcare services, frail persons homes, as well as many other economical and societal sectors.</p>

<b>Which have been the biggest challenges?</b>

<p>The biggest challenge is the seamless integration of existing and new technologies into a reliable architecture. Particularly, the transparency of the teleoperation is extremely challenging in anthropomorphic robotic systems. When using a humanoid robot as an avatar, the sensory feedback scales in complexity. For example, how do you provide the dynanmic balance awareness to the human operator? How do you alleviate the lack of degrees of freedom? How do you conceive a reliable loco-manipulation shared control?</p>
"
- id: visadapt
  title: "Visadapt"
  longtitle: "CNRS INSIS PEPS Visadapt - Adaptative Catadioptric Vision"
  year-start: 2021
  year-end: 2022
  active: false
  partners: [inria,polytechlille,upjv]
  description: "
<p>Visadapt is an exploratory project funded by the PEPS &ldquo;Mechanics of the future&rdquo; program of CNRS&rsquo; Institute of Engineering and Systems Sciences (INSIS). In this project, CNRS-AIST JRL is partnering with Inria and Polytech Lille members of CRIStAL (France) and UPJV, MIS lab (France) with the aim of creating a new camera of highly non-linear field-of-view, enabled by soft robot mirror. The interest is to automatically adapt the resolution of the camera to the scene content, i.e. low in uniform or not interesting areas but high in informative areas or interesting regarding the application (e.g. body, face, geometric features, etc).</p>
    <h4>The goals are:</h4>
    <ul>
      <li>To design a new mirror material capable of deformations while keeping high quality reflection</li>
      <li>To model and compute efficiently the desired shape of the mirror soft robot as function of a field-of-view distribution</li>
			<li>To control the deformation from visual input</li>
    </ul>
<p>A realistic simulator and a first prototype are the expected results.
</p>
"
- id: hadros
  title: "HaDROs"
  longtitle: "HaDROs - HDR panoramic vision for SLAM in scenes of large range of radiance"
  year-start: 2021
  year-end: 2023
  funding: CNRS GDR ISIS
  active: false
  partners: [insarennes,upjv]
  description: "
<p>HaDROs is an exploratory project funded by CNRS&rsquo; Special Interest Group on Information, Signal, Image and Vision (GDR ISIS). In this project, CNRS-AIST JRL is partnering with INSA members of IRISA (France) and UPJV, MIS lab (France) with the aim of enabling visual Simultaneous Localization And Mapping (SLAM) in  environments of challenging lighting, i.e. where we can find both dark shadowed and very bright areas at once. HaDROs leverages the recently introduced panoramic High Dynamic Range (HDR) real-time camera <a href=https://hal.archives-ouvertes.fr/hal-03130948 target=_blank>HDROmni</a> and works on multi-exposure / multi-map SLAM.</p>
    <h4>The goals are:</h4>
    <ul>
      <li>To accurately calibrate the HDROmni camera despite the various exposures acuired at once</li>
      <li>To generalize the multi-map SLAM to multi-exposure</li>
			<li>To enable assistive visual feedback for electric powered wheelchair users even when the surrounding illumination is challenging</li>
    </ul>
<p>The HaDROs system will be integrated on an assistive wheelchair of INSA Rennes (France) and assessed with real users. 
</p>
"
- id: dvsstraight
  title: "DVS-Straight"
  longtitle: "DVS-Straight: Direct Visual Servoing of robot with optimal 3D trajectories"
  pi: carong
  year-start: "2021.10"
  year-end: 2025.3
  funding: AIST ITH
  active: false
  partners: [utbm,upjv]
  description: "
<p>DVS-Straight is an international collaboration project funded by the ITH department of AIST. In this project, CNRS-AIST JRL is partnering with UTBM, CIAD lab (France) and UPJV, MIS lab (France) with the aim of mastering the robot trajectories during image-based visual servoing directly using the dense brightness of captured images as input of the control law (Direct Visual Servoing: DVS). Indeed, DVS is first known for its submillimetric precision and recently for a quite large convergence domain, e.g. with <a href=https://hal.archives-ouvertes.fr/hal-03155667 target=_blank>DDVS</a>. However, trajectories are not always satisfactory.</p>
    <h4>The goals are:</h4>
    <ul>
      <li>To develop an international network of labs and researchers to work on the DVS-Straight topic</li>
      <li>To exploit better the control theory for improving the conditioning</li>
			<li>To encapsulate DVS in multi-objective control frameworks for condisering explicit trajectory constraints</li>
			<li>To research the optimal field-of-view maximizing the motion perceptibility</li>
    </ul>
all for better behavior of DVS-based control laws and hence applicability in a much larger variety of scenarios and contexts. 
<p>A series of international workshops will be organized to share the research, the results and the open-source software as well as to develop the research community on these topics.
</p>
"
- id: comanoid
  title: "COMANOID"
  longtitle: "Multi-Contact Collaborative Humanoids in Aircraft Manufacturing"
  year-start: 2015
  year-end: 2019
  active: false
  url: "http://www.comanoid.eu"
  partners: [cnrs, dlr, inria, sapienza, airbus-gi]
  description: "<p>COMANOID stands for -Multi-Contact Collaborative Humanoids in Aircraft Manufacturing- is a RIA four-year European research project that started in January 2015 as part of the Horizon H2020 program.</p>

  <p>COMANOID aims at deploying humanoid robots to achieve non-added value tasks that have been identified by Airbus Group in aircraft assembly operations. The project focuses on showing precise accessibility (namely into areas where wheeled robots cannot be deployed) through whole body multi-contact planning motion with advanced embedded 3D dense SLAM localization and visuo-force servoing capabilities. Because the robots evolve in human worker co-localized spaces, safety issues will be specifically accounted for. The results of COMANOID will be showcased in a 1:1 scale demonstrator of a real aircraft using two humanoid robots: the HRP-4 position controlled humanoid robot provided by CNRS partner and the TORO torque controlled humanoid robot provided by DLR partner.</p>
    <h4>Goals</h4>
    <ul>
      <li>To investigate the deployment of humanoid robots solutions in Airbus assembly operations that are laborious and inaccessible for wheeled platforms.</li>
      <li>To achieve Human-robot safe and operationally efficient collaboration.</li>
    </ul>"
- id: koroibot
  title: "KoroiBot"
  longtitle: "Improving humanoid walking capabilities by human-inspired mathematical models, optimization and learning"
  year-start: 2013
  year-end: 2016
  active: false
  url: "http://www.koroibot.eu/"
  partners: [heidelberg,cnrs,kit,iit,tudelft,wis,utubingen]
  description: "<p>Teaching two-legged robots a stable, robust “human” way of walking – this is the goal of the international research project KoroiBot, which is named after Koroibos of Elis, the first recorded Olympic champion in history. KoroiBot is funded by the European Union from October 2013 on for three years and gathers an interdisciplinary consortium from the areas of robotics, mathematics and cognitive sciences.</p>
  <p>Whether as rescuers in disaster areas, household helps or as “colleagues” in modern work environments: there are numerous possible areas of deployment for humanoid robots in the future. One of the major challenges on the way is to enable robots to move on two legs in different situations, without falling – even in unknown rough terrain and under possible perturbations.</p>
    <p>In the KoroiBot project, we will study the way humans walk e.g. on stairs and slopes, on soft and slippery ground or over beams and seesaws, and create mathematical models. Besides developing new optimization and learning methods for walking on two legs, we  aim to implement them in practice on real robots. In addition, the research results are to flow into planning new design principles for the next generation of robots.</p>
    <p>Besides the targeted use in robotics, we expect possible applications of the methods in medicine, e.g. for controlling intelligent artificial limbs, designing and controlling exoskeletons as well as in computer animation and in game design.</p>

    <p>Financial support by the European Union within the 7th Framework Program under Grant Agreement No. 611909 is gratefully acknowledged.</p>"
- id: robohow
  title: "RoboHow"
  longtitle: "RoboHow"
  year-start: 2012
  year-end: 2016
  active: false
  url: "https://robohow.eu/"
  partners: [bremen,cnrs,epfl,kth,kul,forth,ul,aldebaran,tum]
  description: "<p>Robohow is a four-year European research project that started in February 2012. It aims at enabling robots to competently perform everyday human-scale manipulation activities - both in human working and living environments.</p>
  <p>In order to achieve this goal, Robohow pursues a knowledge-enabled and plan-based approach to robot programming and control.</p>
  <p>The vision of the project is that of a cognitive robot that autonomously performs complex everyday manipulation tasks and extends its repertoire of such by acquiring new skills using web-enabled and experience-based learning as well as by observing humans.</p>"
  videos:
    - title: "Human-Humanoid Joint Haptic Table Carrying Task with Height Stabilization using Vision"
      url: "https://www.youtube.com/watch?v=LnuF41CKq0E"
      img: "http://i1.ytimg.com/vi/LnuF41CKq0E/mqdefault.jpg"
    - title: "Proactivity and Role Switching in a Humanoid-Human Transportation Task"
      url: "https://www.youtube.com/watch?v=kYUdYOIYeZ0"
      img: "http://i1.ytimg.com/vi/kYUdYOIYeZ0/mqdefault.jpg"
- id: vere
  title: "VERE"
  longtitle: "Virtual Embodiment and Robotic Re-Embodiment"
  year-start: 2010
  year-end: 2015
  active: false
  url: "http://www.vereproject.eu/"
  description: "<p>This Integrated Project aims at dissolving the boundary between the human body and surrogate representations in immersive virtual reality and physical reality. Dissolving the boundary means that people have the illusion that their surrogate representation is their own body, and act and have thoughts that correspond to this. The work in VERE may be thought of as applied presence research and applied cognitive neuroscience, and it would also significantly add to scientific knowledge in these areas.</p>"
  videos:
    - title: "Humanoid robot embodiment with a brain-computer interface"
      url: "https://www.youtube.com/watch?v=ekP1oxD6Vj0"
      img: "http://i1.ytimg.com/vi/ekP1oxD6Vj0/mqdefault.jpg"
    - title: "Multitask Humanoid Control with a Brain-Computer Interface: user experiment with HRP-2"
      url: "https://www.youtube.com/watch?v=6bynYqmVVck"
      img: "http://i1.ytimg.com/vi/6bynYqmVVck/mqdefault.jpg"
- id: robot_at_cwe
  title: "Robot@CWE"
  longtitle: "Robot@CWE"
  year-start: 2007
  year-end: 2009
  active: false
  description: "<p>The main objective of this STREP is to research and demonstrate integrative concepts of advanced robotic systems, to be seen as collaborative agents, in various environments working together with humans. We will integrate collaborative robotic systems as active agent operated through various control paradigms within working environment clusters. ROBOT@CWE will design suitable architectures and technologies to achieve this goal. Current developments in robotics tend toward human-centred design raising issues in: human-machine interface, human-robot interaction, working robots, assistive robots, ubiquitous robotics,humanitarian robotics for disaster support and rescue mission, European security, etc.</p>"
- id: kakenhiG_2015
  title: "Sixth Finger"
  longtitle: "Towards natural learning of an additional effector by the human brain"
  year-start: 2015
  year-end: 2017
  active: false
  description: >
    <p>Our brain can readily learn to control new actions with our effectors/limbs, but can it also learn to control new limbs in addition to our current ones? Answering this question will clarify an individual brain’s neuronal limits, for functional augmentation with brain-machine interface (BMI) systems and of plasticity after brain damage. This JSPS (Kakenhi 'houga')multi-disciplinary project with the University of Electro-communication in Tokyo will provide a crucial understanding of the <i>unused capabilities</i> of the human brain in regard to effector control and will enable a human to use this capability to incorporate an additional artificial limb naturally into his body schema.</p>
- id: kakenhiG_2013
  title: "AnxCon"
  longtitle: "Understanding and controlling the effects of anxiety on motor behavior"
  year-start: 2013
  year-end: 2016
  active: false
  description: >
    <p>This JSPS ('Kakenhi Kiban B') project investigates the effects of cognitive anxiety associated with observation of other individuals and competition on one's own motor behavior. In Collboration with <a href="https://cinet.jp/english/" target="_self">Centre for Information and Neural Networks (CINET)</a> in Osaka, <a href="http://www.nifs-k.ac.jp/" target="_self">National Institute of Fitness and Sports</a> in Kanoya, and <a href="http://www.chukyo-u.ac.jp/" target="_self">Nagoya Chukyo University</a>, we are using cognitive neuroscience experiments and computational modelling to acquire a quantitative measure of motor deteriorations due to anxiety, and investigate how non-human agents like robots may be used to influence human behaviors.</p>
- id: jst_mirai
  title: "CareAvatar"
  longtitle: "Remotification of physical interaction for resilient society"
  longtitle-jp: "物理的接触の遠隔化によるレジリエントな社会の実現"
  year-start: "2021.10"
  year-end: 2024.3
  funding: JST-Mirai Program
  funding-jp: JST未来社会創造事業
  active: false
  description: >
    <p>We aim to create a society that is resilient to infectious diseases, by using avatar robots to enable telework for essential workers whose work requires physical contact with people. ​<br>We will conduct research and development of a tele-physical caregiving service that enables telework in caregiving.​<br>In this service, a caregiver operates a nursing care avatar at the nursing care site from a remote location via a digital twin of the nursing care site built in cyberspace to perform nursing care tasks. To realize this service, we will conduct research and development on the following three technologies.​<br><ol><li>Haptic sensing technology that captures complex physical information as a distribution on a three-dimensional curved surface,</li><li>Technology for understanding and reproducing the movements of a remote caregiver, and</li><li>Safety standards that are essential for social acceptance.​</li></ol></p>
  description-jp: >
    <p>介護のテレワーク化を可能とする遠隔物理介護サービスの研究開発を行う。このサービスは、介護者が遠隔地からサイバー空間に構築された介護現場のデジタルツインを介して、介護現場にある介護アバターを操作して介護業務を実施するものである。このサービス実現のため、<br><ol><li>接触状態をデジタルツインに反映するための複合的な物理情報を３次元曲面上の分布として捉える触覚センシング技術、</li><li>サイバー空間を介して、遠隔にいる介護者の動作を理解し、介護現場にある介護アバターで再現する動作理解・再現技術、</li><li>介護アバターが社会に受容される上で不可欠な安全基準の研究開発</li></ol>を実施する。</p>
- id: kakenhi_wakate2019_kumagai
  title: "TaskPlanning"
  longtitle: "Development of task planning system for autonomous task execution"
  longtitle-jp: "半未知環境の環境構成物記憶と作業依存関係に基づくヒューマノイドの作業手順計画法"
  year-start: 2019.4
  year-end: 2023.3
  funding: JSPS Grant-in-Aid for Early-Career Scientists
  funding-jp: 日本学術振興会 科学研究費助成事業（科研費） 若手研究
  active: false
  description: >
    <p>This project is a four-year research project funded by JSPS Kakenhi Grant-in-Aid for Early-Career Scientists (19K20380). Our ultimate goal is enabling a robot to autonomously generate and perform a sequence of target tasks considering task dependencies, constraints of a robot and the surrounding environment.​ To achieve this goal, we will conduct research and development on the following three topics.​<br><ol><li>Task management system which can perform error recovery and parallel processing of a task sequence based on the dependency of tasks.</li><li>Motion planning technology which can generate feasible whole-body motion for a robot to reach the destination and perform required tasks considering kinematics and dynamics constraints.</li><li>Semantic environmental memorization to find objects which are neccesary for task execution.​</li></ol></p>
- id: ryouikikoubo_2021
  title: "Tuning-free control"
  longtitle: "Data-driven online adaptive model/control parameters for tuning-free motion generation"
  year-start: 2021
  year-end: 2023
  funding: AIST ITH
  active: false
  description: "
    <p> Controlling the dynamics of complex systems requires identifying and tuning more and more parameters, leading to increasing adaptation times for new tasks or new robots. In this project, we plan to make this process automatic and online to constantly improve the performance during the robot operation. Classical control techniques, together with statistical tools and machine learning will be exploited to this end.</p>"
- id: iam
  title: "I.AM."
  longtitle: "Impact Aware Manipulation by Dexterous Robot Control and Learning in Dynamic Semi-Structured Logistic Environments"
  year-start: 2020
  year-end: 2024
  funding: H2020
  active: false
  url: "https://i-am-project.eu/"
  partners: [tue, algoryx, frankaemika, smartrobotics, vanderlande, cnrs,epfl,tum]
  description: >
    <p> Europe is leading the market of torque-controlled robots. These robots can withstand physical interaction with the environment, including impacts, while providing accurate sensing and actuation capabilities. I.AM. leverages this technology and strengthens European leadership by endowing robots to exploit intentional impacts for manipulation. I.AM. focuses on impact aware manipulation in logistics, a new area of application for robotics which will grow exponentially in the coming years, due to socio-economical drivers such as booming of e-commerce and scarcity of labor.</p>

    <p>Learn more about it on the <a href="https://i-am-project.eu/">project website</a>.
- id: icph
  title: "iCPH"
  longtitle: "Interactive cyber-physical human: Generating contact-rich whole-body motions"
  longtitle-jp: "サイバーフィジカルヒューマンによる全身接触運動の包括的データ駆動学習・予測・生成"
  pi: morisawa
  year-start: 2022.4
  year-end: 2027.3
  funding: JSPS Grant-in-Aid for Scientific Research (S)
  funding-jp: 日本学術振興会 科学研究費助成事業（科研費） 基盤研究(S)
  active: true
  partners: [tus]
  description: >
    <p>ulo-skeletal systems have been developed. Placing human measurement and model-based robotic approaches as an important basis, our challenge is to overcome this difficulty also by exploiting a powerful machine learning framework to benefit from a large dataset of human motions. Consequently, we can take advantage of humanoids’ ability to interact with the physical world to refine and validate the model of the motion strategy and controller, as well as digital actors’ flexibility to change many parameters to simulate and learn motions with various shapes, dimensions, and physical models in different environments. Then, we hope to come up with a system that predicts and synthesizes human motions, notably motions involving complex contacts, in a variety of environments. As the cyber-physical human evolves, we expect it can be utilized to design ergonomic products, create robots that can support human comfortably by estimating human intention, and devise a humanoid robot that can coexist with humans naturally and safely in their proximity.</p>
  description-jp: >
    <p>接触こそが人型システムの運動記述の鍵であるとの新視点から、接触運動の継続学習・予測・生成を行う統一的な方法論の確立を目指す。人は環境と接触を伴う運動を自然に行うのに対し、その機序の解明や工学的利用に関する未解決課題は多い。それへの挑戦の一環として、申請者らは解析微分により人型システムの運動理解・最適化を行う革新的な理論基盤を構築してきた。本課題では、新たに ①サイバーフィジカルヒューマン（シミュレーション・実世界の人間・ロボット統合モデル）によるデータ取得・拡張の枠組と接触運動の一般的表現の提案、②運動逆最適化に基づく接触運動ネットワークの継続学習とデータベース構築、③逆予見制御とベクトル量子化学習の融合によるシンボル体系化と予測に包括的に取り組み、接触を伴う任意の運動の理解と生成に挑む。データベース公開により内外機関と連携を推進するとともに、分布型衣服埋め込みセンサによる作業者負担の実時間モニタリング、ロボットと人の遠隔協調による重量物運搬・組付け作業など、各種産業での生産性やQoWの向上にインパクトをもたらす実問題で成果を検証し、複雑な人の運動を予測し自然に支援するシステムを創出する。</p>
- id: icps
  title: "ICPS"
  longtitle: "Industrial Cyber-Physical Systems"
  longtitle-jp: "インダストリアルCPS"
  pi: kanehiro
  year-start: 2020.4
  year-end: 2025.3
  funding: AIST
  active: false
  description: >
    <p>The target is to solve the problem of declining birthrate and aging population, which is one of the social issues. We aim to realize the following items in all industrial fields by using technology that integrates AI, robots, sensors, etc. to cooperate with humans.</p>
    <ul>
      <li>Optimization of labor input resources</li>
      <li>Improvement of employee quality of work</li>
      <li>Creation of new customer value, inheritance of skills and sophistication in anticipation of changes in the industrial structure</li>
    </ul>
    <p>We improve the technology of modeling, planning / control, system design, etc. for systems consisting of workers, sensors, robots, equipment, work environment, etc. in the manufacturing or service industry. By utilizing AI that cooperates with humans, we develop and demonstrate technology that optimizes from the viewpoint of workability and productivity while maintaining the safety and flexibility of the system. In order to develop new technologies in the real area, we have established a testbed environment in the Cyber Physical System research building at the AIST Tokyo Waterfront, including the factory environment in the manufacturing field and the retail store environment in the logistics field, and are promoting industry-academia-government collaboration activities through collaborative research and consortium partnerships.</p>
  description-jp: >
    <p>社会課題の一つである少子高齢化問題の解決をターゲットと定め、サービス業を含む全ての産業分野で労働等の投入資源の最適化、従業員のQuality of Work(QoW)の向上、産業構造の変化を先取する新たな顧客価値の創出、技能の継承・高度化に向けて、人と協調する人工知能（AI）、ロボット、センサ（IoT）等を融合した技術を開発することを目標としています。</p>
    <p>本研究センターでは特に、製造業やサービス業等の現場における人、センシング、ロボット、機器、作業環境等から構成されるシステムに関して、サイバーフィジカルシステム（CPS）を基盤として、モデリング、計画・制御、システム設計等の技術を高度化・統合化するとともに、人と協調するAIを活用することにより、当該システムの安全性と柔軟性を保ちつつ作業性や生産性の観点から最適化する技術を開発し実証します。加えて、実現場での展開を目指す上で、臨海副都心センターCPS研究棟内に新たな技術を試すことを可能とするテストベッド環境として製造分野における工場環境、物流分野における小売店舗環境等を構築しており、共同研究、コンソーシアム連携による産学官連携活動を推進しています。</p>
- id: kakenhi_kibanb2021_morisawa
  title: "ACLoMa"
  longtitle: "Toward autonomous and continuous locomotion and manipulation in confined environments"
  longtitle-jp: "複雑未知環境下における即時動作を可能とする多点接触運動システムの実現"
  year-start: 2021.4
  year-end: 2024.3
  funding: JSPS Grant-in-Aid for Scientific Research (B)
  funding-jp: 日本学術振興会 科学研究費助成事業（科研費） 基盤研究(B)
  active: false
  description: >
    <p>The aim of this project is to develop an unified framework of autonomous and continuous locomotion and manipulation in a constrained environment by robots which can work in repetitive, tedious, or a heavy task for humans in large-scale manufacturing.
    <p>Due to a huge search space and many constraints in such an environment, it is hard to find a feasible motion.</p>
    <p>In order to solve this problem, the motion libraries of locomotion and manipulation will be prepared from the environment primitives and the whole body trajectories corresponding to enriched contact conditions (e.g. sliding) from a virtual environment in a dynamics simulator in advance by using a sort of machine learning.</p>
    <p>Then the continuous locomotion and manipulation without interruption will be archived by executing environment measurement/recognition, a whole-body motion planning and its controller in frequent cycle from the motion libraries and sensing data.</p>
    <p>A whole-body controller will be also developed in order to adapt a real-environment by considering measurement and unmodeled errors under the constraints by introducing the knowledge of non-linear control schemes.</p>
    <p>In this way, we propose a new framework of locomotion and manipulation for robots to expand the reachable area and acquire the skills to perform new tasks.</p>
  description-jp: >
    <p>本研究課題では、ロボットが狭隘度の高い複雑未知環境における3次元自律移動を実現する。このような環境は探索空間が広いため、計算コストの面から環境計測情報から逐次的に接触位置計画や全身動作生成を行うことが困難である。本研究では、これまでに我々のチームで開発してきた多点接触運動計画・制御技術を応用して、様々な仮想環境からロボットの接触状態に対応した環境のプリミティブや全身軌道を事前に動力学シミュレータで機械学習の手法を用いてモーションライブラリを構築する。実環境において環境計測情報やロボットの状態を基にモーションライブラリから即時的に全身軌道を生成する。また重心動力学方程式に着目し、全ての運動を統一的に扱うことのできる多点接触運動制御フレームワークを開発し、狭隘環境を含む実環境で自律移動を実証する。</p>
- id: jrp_khi
  title: "Friends"
  longtitle: "Joint Research on Airplane Manufacturing Humanoid Robot"
  pi: kanehiro
  year-start: 2020.4
  year-end: 2025.3
  active: true
  partners: [khi]
  description: >
    <p>This Joint Research aims to develop a humanoid-type robot for airplane manufacturing which can relieve human workers from non-added-value tasks.</p>
    <p>Under this Joint Research, the humanoid-type robot for airplane manufacturing will be developed by customizing KHI's current humanoid-type robot ("KHI's humanoid robot") for an airplane manufacturing purpose and the objective of this Joint Research is such customization.</p>
    <p>The humanoid-type robot for airplane manufacturing will be designed based on requirements obtained in previous researches with an airplane manufacturing company. It must be a slim, lightweight and multi-limbed body such that it can maneuver in confined and cluttered spaces and conduct precision tasks with high-norm of safety and certifications. The developed humanoid-type robot for airplane manufacturing will be evaluated through use-cases given by potential users and remodeled several times to meet the requirements.</p>
- id: sigrole
  title: SIGRoLe
  longtitle: "Self-Improving AI for seamless human-guided robot learning"
  pi: cisneros
  year-start: 2023.11
  year-end: 2026.3
  active: true
  partners: [cmu]
  description: >
    <p>Robots can alleviate the burden of repetitive and dangerous tasks. Such robots need to be autonomous in a low-level but able to follow high-level human guidance. The interface must be such that tasks that take much burden can be semi-automatic (through shared control), leaving the high-level planning to the user through teleoperation. Collaboration of robot learning, and human guidance is necessary for solving unstructured problems. We propose a general shared autonomy framework (not targeted to specific robotic system) that will rely on Large Language Models (LLM) and propositional logic for action inference and predicting user intention, relying on an AI framework for continuously learning how to perform tasks.</p>
- id: hican
  title: HICAN
  longtitle: "Human-Inclusive Dynamic Control and Navigation for Next-Gen Robotics "
  pi: benalleguem
  year-start: 2023.11
  year-end: 2026.3
  active: true
  partners: [cmu]
  description: >
    <p>This study aims to bridge the gap between precise yet dangerous industrial robots and safer but less efficient collaboration counterparts by developing a unified advanced control and navigation framework. We aim to integrate this research into our existing mc_rtc control environment, building on our current control system used for various robots and tasks. Inspired by the need for industry certifications, our focus is on creating a complete system that enables robots to move and navigate more autonomously and safely in dynamic environments, including around humans. The foundation of this framework relies on theoretical guarantees of stability and convergence provided by control theory, those of optimality and performance provided by numerical optimization techniques, and the reliability provided by sensor fusion. We propose also to establish systematic benchmarking on actual robotic platforms. Finally, we aim for actual industrial applications in physical tasks and improved robot autonomy. The target robots are any multi-body robot (fixed and mobile manipulators, legged robots, etc.)</p>
- id: kakenhi_wakate2023_kumagai
  title: "RLOptMCP"
  longtitle: "Development of multi-contact motion planning based on learned future contact feasibility"
  longtitle-jp: "接触の将来性学習に基づく多点接触動作計画の実現"
  pi: kumagai
  year-start: 2023.4
  year-end: 2026.3
  funding: JSPS Grant-in-Aid for Early-Career Scientists
  funding-jp: 日本学術振興会 科学研究費助成事業（科研費） 若手研究
  active: true
  description: >
    <p>The goal of this project is to develop a framework to plan multi-contact motion for a humanoid robot to perform complex tasks such as those done by human workers in large-scale manufacturing. To achieve this goal, we need to efficiently find feasible contacts from an infinite number of candidates in a confined environment. We solve this issue by focusing on future contact feasibility, which quantitatively represents how much it improves the solvability of the optimization problems for motion planning and the quality of the resulting motion. We introduce the reinforcement learning approach to predict it over the future and integrate it with optimization-based motion planning that explicitly considers the kinematics and dynamics of a humanoid robot, which enables us to generate feasible whole-body motion that is advantageous to satisfy necessary constraints over the future. We consider that this research will clarify how a humanoid robot obtains empirical intelligence to interact with an environment considering the tasks and its body structures.</p>
- id: impadm
  title: ImpAdm
  longtitle: "Impact/Overload Management for Admittance-Type Mechatronic Devices Using Nonsmooth Controllers"
  longtitle-jp: "アドミッタンス型メカトロ機器のための非平滑制御則を用いた撃力・過荷重マネジメント"
  pi: cisneros
  year-start: 2024.4
  year-end: 2027.3
  funding: JSPS Grant-in-Aid for Scientific Research (B)
  funding-jp: 日本学術振興会 科学研究費助成事業（科研費） 基盤研究(B)
  active: true
  partners: [hu]
  description: >
    <p>Many of today's industrial robotic equipment is controlled by dedicated servo amplifiers and operates based on position commands (or speed commands) from a higher-level controller. In applications where it's necessary to manipulate contact force, a method called admittance control, which modifies position commands based on information from force sensors or external force observers, is commonly used. Therefore, many industrial mechatronics devices can be described as "admittance-type," which means they are force-input and position-output systems. These devices are technically and cost-effectively mature, but they are not suited for situations where contact/non-contact changes frequently, especially where excessive force or impact might occur. This research project aims to establish a force control strategy for existing hardware of industrial and humanoid robots when subjected to overload or impact forces. To achieve this goal, a framework of nonsmooth control laws that can explicitly handle the saturation of state variables and control inputs will be utilized.</p>
  description-jp: >
    <p>今日の産業用ロボット機器の多くは専用のサーボアンプで制御されており，上位コントローラからの位置指令（あるいは速度指令）にもとづいて動作する．接触力を操作する用途においては，力センサや外力オブザーバからの情報にもとづいて位置指令を修正するアドミッタンス制御とよばれる手法がよく用いられる．このため，多くの産業用メカトロニクス機器は，力入力・位置出力型の「アドミッタンス型」であるといえる．これらの機器は技術的にもコスト的にも成熟しているが，接触・非接触が頻繁に変化する状況，特に，過大な力や撃力が加わりうる状況に不向きである．本研究課題では，産業用ロボットや人型ロボットの既存のハードウェアを対象として，過荷重や撃力が加わる際の力制御の戦略を確立する．この目的のために，状態量や制御入力の飽和を陽に扱える非平滑制御則の枠組みを活用する.</p>
- id: post5g
  title: SDP4ESI
  longtitle: "Development of a software development platform that enhances quality, reliability, and safety for improving the efficiency of robot system integration"
  longtitle-jp: "ロボットSI効率化に向けた品質・信頼性・安全性強化型ソフトウェア開発基盤の構築"
  pi: kanehiro
  year-start: 2025.7
  year-end: 2028.2
  funding: NEDO
  funding-jp: NEDO
  active: true
  partners: [esol,robocip,sec,panasonic,fujisoft,mamezo,ritsumeikan]
  description: >
    <p>By leveraging open-source software (OSS) and defining a modular robot architecture, we aim to establish a robot system development methodology that enables flexible and low-cost deployment even in long-tail application domains. To this end, we will develop a screening system for validating OSS and modules developed by various vendors, tools to improve the efficiency of individual module development, reference robot systems for verification, as well as various development-support tools, simulators, and simulation models. Through these efforts, we will provide reusable and reliable software modules and a development platform that enables their effective use, thereby allowing the development of high-quality robot systems without requiring highly specialized expertise. Furthermore, we aim to provide these software modules, tools, and related assets through a distribution platform to be established in the future.
  description-jp: >
    <p>OSS活用とモジュールベースのロボットアーキテクチャの定義を通じて、ロングテール領域にも柔軟かつ低コストで導入可能なロボットシステム開発手法を確立する。OSS や様々なベンダが開発するモジュールを検証するためのスクリーニングシステム、個々のモジュール開発を効率化するツールや検証用リファレンスロボットシステム、その他様々な開発を支援するツール・シミュレータやシミュレーションモデルを開発することで、安心して再利用可能なソフトウェアモジュールや、それらを利用可能な開発基盤を提供し、高度な熟練者でなくとも高品質なロボットシステム開発を可能にするとともに、将来的に構築されるソフトウェアモジュールやツール等の流通基盤上での提供を目指す。
- id: moral
  title: MORAL
  longtitle: "MORAL: A Body-Thinking Robot Foundation Model"
  longtitle-jp: "身体が考えるロボット基盤モデル：MORAL"
  pi: murooka
  year-start: "2025.10"
  year-end: 2027.3
  funding: JST CREST
  funding-jp: JST CREST
  active: true
  partners: []
  description: >
    <p>Current vision-language-action approaches are dominated by symbolic information such as text, lacking reasoning grounded in body structure or internal states. We propose MORAL as a concept for body-centered robot intelligence, placing bodily representations—including morphology—at the core while integrating modalities such as auditory sensing. This concept envisions embodied reasoning and action generation beyond symbols, aiming for future capabilities such as handling unseen situations, adapting force and motion, and generalizing across different morphologies, with the goal of enabling robots to flexibly operate in diverse real-world tasks in manufacturing.
  description-jp: >
    <p>現在の視覚・言語・行動モデルは記号的情報に偏り、身体構造や内部状態に基づく推論を欠いている。本研究では、形態を含む身体表現を中核に据え、聴覚などの身体性モダリティを統合する身体中心の知能モデルMORALを提案する。MORALは記号を超えた身体的推論と行動生成を構想し、未知の状況への対応、力加減の適応、身体構造間の汎化を目指し、多様な実環境タスクに柔軟に適応するロボットの実現を目的とする。

