---
layout: default
title: CNRS-AIST JRL - Humanoid Lab
---

<div class="breadcrumbs-container">
<div class="container">
  <div class="row">
    <div class="col-lg-12">
      <ol class="breadcrumb">
        <li><a href="/index_en.html">Home</a>
        </li><li class="active">ヒューマノイド研究室 (筑波大学)</li></ol>
      <h1 class="page-header">ヒューマノイド研究室
	<small>
	[<a href="{{site.baseurl}}/en/humanoid_lab.html">EN</a>
	/
	<a href="{{site.baseurl}}/en/humanoid_lab_jp.html">JP</a>]
	  <br/>
	  <a href="https://www.tsukuba.ac.jp/education/g-courses-g-list-prev-cooperatives/" target="blank_">筑波大学 - 連携大学院 <span class="glyphicon glyphicon-globe" style="font-size: small;"></span></a></small></h1>
    </div>
  </div>
</div>
</div>


<!-- Page Content -->
<div class="container">

  <table width="900" border="0" align="center" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
	<td style="padding: 20px; border-radius:15px">
      <table border="0" width="100%" cellpadding="7">
        <tbody>
          <tr>
            <td width="330" align="left" rowspan="2">
	      <img src="{{site.baseurl}}/en/assets/humanoidlab/four_humanoids.jpg" width="300" style="border-radius:15px">
	      <br>
	      <br>
	      <img src="{{site.baseurl}}/en/assets/humanoidlab/combined_logo.png" width="300" style="border-radius:15px">
	    </td>
            <td>
	      <p align="justify">
		ヒューマノイドラボは、筑波大学メインキャンパスから約5kmの産総研にあるCNRS-AIST JRLの一研究室です。金広教授（産総研連携大学院）の指導のもと、本学の大学院生が研修生・RAとしてJRLで働くことができる連携大学院制度があります。
	      </p>
	      <p align="justify">
		この研究室では、大学院生が国内外の研究者とともに、さまざまなロボットプラットフォームや研究テーマに取り組むことができるユニークな機会を提供しています。主な研究テーマは、タスクやモーションの計画・制御、知覚を介した人間や周辺環境とのマルチモーダルインタラクション、認知ロボティクスなどです。私たちの研究室のほとんどのメンバーはバイリンガルなので、日本語を話す学生だけでなく、英語を話す学生も研究室に参加することを推奨しています。
	      </p>
	    </td>
	    <td valign="top" align="center" style="padding-left: 20px;padding-bottom: 20px" width="150" >
	      <a href="{{site.baseurl}}/en/members/member-kanehiro.html">
	      <img src="{{site.baseurl}}/en/assets/members/kanehiro.jpg" width="100" style="border-radius:50px">
	      </a>
	      <p align="center">KANEHIRO Fumio<br>
		<b>金広 文男</b></p>
	      <p style="margin-top: -0.5em; font-size:80%"><i>
	      f-kanehiro_*_aist.go.jp</i></p>
	      <p style="margin-top: -0.5em; background-color:#e0f0fe">教授</p>
	    </td>
	  </tr>
	  <tr>
	    <td colspan="2">
	      <p align="justify">
		当研究室では、常に優秀で意欲的な大学院生を募集しています。筑波大学大学院システム情報科学研究科知能機械システム専攻修士課程または博士課程に正規の入学手続き（夏と冬に試験を実施）で合格した方のみです。</p>

	      <p align="justify"><b>
		  興味のある方は、出願手続きを始める前に、研究室または金広教授に直接お問い合わせください。</b></p>

	      <!-- <p>Check out our YouTube channel <a href="https://www.youtube.com/channel/UCYwHCdMHAKYZJ2MQIoTavVQ"> here. </a></p> -->
	      <p align="justify">(このページは、在校生によって管理されています。)</p>
	    </td
	  </tr>
        </tbody>
      </table>

      <div class="row">
	<h3 class="page-header">研究内容</h3>
	  <!-- content -->
	  <table border="0" width="100%" cellpadding="17" cellspacing="0">
            <tbody>
              <tr valign="top">
		<td style="padding: 20px;background-color:#e0f0fe; border-radius:15px">
		  <table border="0" width="100%" cellpadding="0" cellspacing="0" >
		    <tbody>
                      <tr>
			<td><b><font size="+1">Simultaneous localization and mapping(SLAM) in dynamic environment</font></b></td>
			<td>&nbsp;</td>
			<td rowspan="3" valign="top" width="200">&nbsp;<img src="{{site.baseurl}}/en/assets/humanoidlab/hl_01_slam.jpg" height="227" border="0"></td>
                      </tr>
                      <tr>
			<td height="10"></td>
			<td height="10"></td>
                      </tr>
                      <tr>
			<td valign="top">Nowadays, SLAM in the dynamic environment has become a popular topic.
			  This problem is called dynamic SLAM where many solutions have been proposed to segment
			  out the dynamic objects that bring errors to camera tracking and subsequent 3D reconstruction.
			  However, state-of-the-art dynamic SLAM methods face the problems of accuracy and speed, which
			  is due to the fact that one segmentation algorithm cannot guarantee both points at the same time.
			  We propose a multi-purpose dynamic SLAM framework to provide a variety of selections for segmentation,
			  each has its applicable scene. Besides, if the user selects the semantic segmentation, the object-oriented
			  semantic mapping is beneficial for high level robotic tasks. </td>
			<td width="50">&nbsp;</td>
                      </tr>
		    </tbody>
		  </table>
		</td>
              </tr>
            </tbody>
	  </table>

	  <br>
	  <br>

	  <!-- content -->
	  <table border="0" width="100%" cellpadding="17" cellspacing="0">
            <tbody>
              <tr valign="top">
		<td style="padding: 20px;background-color:#e0f0fe; border-radius:15px">
		  <table border="0" width="100%" cellpadding="0" cellspacing="0">
		    <tbody>
                      <tr>
			<td><b><font size="+1">6-DoF Object Pose Estimation</font></b></td>
			<td>&nbsp;</td>
			<td rowspan="3" width="312"><img src="{{site.baseurl}}/en/assets/humanoidlab/hl_02.png" width="311" border="0"></td>
                      </tr>
                      <tr>
			<td height="10"></td>
			<td height="10"></td>
                      </tr>
                      <tr>
			<td valign="top">For a humanoid robot to interact with objects in its surrounding environment,
			  it is essential for the robot to find the position and orientation of the object relative to
			  itself - often through the use of its vision sensors. The 3D position and roll, pitch, yaw
			  rotation together comprise the 6 degrees-of-freedom pose of the object. For precise grasping
			  and manipulation of tools, this pose needs to be estimated with a  high degree of accuracy.
			  Further, we desire robustness against challenging lighting conditions, occlusions, and non-availability
			  of dense and accurate object models. This work mainly involves the use of Deep Learning based strategies
			  for solving problems in this sphere.
			</td>
			<td width="50">&nbsp;</td>
                      </tr>
		    </tbody>
		  </table>
		</td>
              </tr>
            </tbody>
	  </table>
		   <br>
	       <br>


		  <table border="0" width="100%" cellpadding="17" cellspacing="0">
            <tbody>
              <tr valign="top">
		<td style="padding: 20px;background-color:#e0f0fe; border-radius:15px">
		  <table border="0" width="100%" cellpadding="0" cellspacing="0">
		    <tbody>
                      <tr>
			<td><b><font size="+1">Enhanced Visual Feedback with Decoupled Viewpoint Control in Immersive Teleoperation using SLAM</font></b></td>
			<td>&nbsp;</td>
			<td rowspan="3" width="312"><img src="{{site.baseurl}}/en/assets/humanoidlab/hl_cslam.png" width="311"  border="0"></td>
                      </tr>
                      <tr>
			<td height="10"></td>
			<td height="10"></td>
                      </tr>
                      <tr>
			<td valign="top">In immersive humanoid robot teleoperation, there are three main shortcomings that can alter
				the transparency of the visual feedback: (i) the lag between the motion of the operator’s and robot’s head.
				This latency could cause a noticeable delay in the visual feedback, which jeopardize the embodiment quality,
				can cause dizziness, and affects interactivity resulting in operator frequent motion pauses for the visual
				feedback to settle; (ii) the mismatch between the real-camera and the headset field-of-views (FOV), the
				robot camera having generally lower FOV; and (iii) a mismatch between the human and head head range of
				motions, that of the robot is also generally lower. In order to leverage these drawbacks, we developed a
				decoupled viewpoint control solution for humanoid platform which allows visual feedback with low-latency
				in virtual reality and artificially increase the camera field-of-view range to match that of the operator
				headset. Our novel solution uses SLAM technology to enhance and complement visual feedback from reconstructed
				mesh and complements the areas that are not covered by the real-time robot’s point cloud visual feedback.
				As a result, the operator is fed with real-time vision from the robot’s head orientation by observing the
				pose of the point cloud. Balancing this kind of awareness and immersion is important in virtual reality
				based teleoperation, considering the safety and robustness of control system.
			</td>
			<td width="50">&nbsp;</td>
                      </tr>
		    </tbody>
		  </table>
		</td>
              </tr>
            </tbody>
	  </table>

      </div>

      <div class="row">
	<h3 class="page-header">構成メンバー</h3>
        <table width="100%" border="0" cellspacing="0" class="table table-striped">
          <tbody>
            <tr>
	      <th width="25%">氏名</th>
	      <th width="25%">学年</th>
	      <th width="25%">メールアドレス<br>
		_*_を@に変えて送信ください</th>
            </tr>
	    <!--
		<tr>
		  <td><a href="{{site.baseurl}}/en/members/member-kanehiro.html">金広 文男</a></td>
		  <td>連携大学院教授</td>
		  <td width="25%">f-kanehiro_*_aist.go.jp</td>
		</tr>
		-->
	    <tr>
	      <td><a href="{{site.baseurl}}/en/members/member-qin.html">覃 毅力</a></td>
	      <td>博士課程4年</td>
	      <td width="25%">yili.tan_*_aist.go.jp</td>
            </tr>
            <tr>
	      <td><a href="{{site.baseurl}}/en/members/member-sun.html">孫 楽源</a></td>
	      <td>博士課程3年</td>
	      <td width="25%">son.leyuansun_*_aist.go.jp</td>
            </tr>
            <tr>
	      <td><a href="{{site.baseurl}}/en/members/member-singh.html">Rohan Pratap Singh</a></td>
	      <td>博士課程2年</td>
	      <td width="25%">rohan-singh_*_aist.go.jp</td>
            </tr>
            <tr>
	      <td>Xinchi Gao</td>
	      <td>修士課程1年</td>
	      <td width="25%"></td>
            </tr>
            <tr>
	      <td>屋宮 友哉</td>
	      <td>修士課程1年</td>
	      <td width="25%"></td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="row">
	<div class="col-lg-12">
	  <h3 class="page-header">連絡先</h3>
          <table width="100%">
            <tbody>
              <tr align="center">
		<td>
		  <iframe src="https://www.google.com/maps/embed?pb=!1m28!1m12!1m3!1d25795.692830871016!2d140.1074103178154!3d36.082234076560354!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!4m13!3e6!4m5!1s0x60220bff99f57b0b%3A0x1cad40e7632fb4b8!2zVW5pdmVyc2l0eSBvZiBUc3VrdWJhIOetkeazouWkpw!3m2!1d36.103866599999996!2d140.1020979!4m5!1s0x60220cc567b824f5%3A0xecc14922713a4044!2z44CSMzA1LTg1NjAgSWJhcmFraSwgVHN1a3ViYSwgVW1lem9ubywgMSBDaG9tZeKIkjEtMSDkuK3lpK7nrKwx44Gk44GP44Gw5pys6YOo5oOF5aCx5oqA6KGT5YWx5ZCM56CU56m25qOfIFRzdWt1YmEgQ2VudGVyLCBBSVNUOiBOYXRpb25hbCBJbnN0aXR1dGUgb2YgQWR2YW5jZWQgSW5kdXN0cmlhbCBTY2llbmNlIGFuZCBUZWNobm9sb2d5!3m2!1d36.0624307!2d140.1356783!5e0!3m2!1sen!2sjp!4v1650429789876!5m2!1sen!2sjp" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
		</td>
              </tr>
            </tbody>
          </table>
	</div>
      </div>

      </td></tr>
    <tbody>
  </table>
</div>
<!-- /.container -->

