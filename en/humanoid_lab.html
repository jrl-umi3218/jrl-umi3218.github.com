---
layout: default
title: CNRS-AIST JRL - Humanoid Lab
---

<div class="breadcrumbs-container">
<div class="container">
  <div class="row">
    <div class="col-lg-12">
      <ol class="breadcrumb">
        <li><a href="/index_en.html">Home</a>
        </li><li class="active">Humanoid Lab (University of Tsukuba)</li></ol>
      <h1 class="page-header">Humanoid Lab
	<small>
	[<a href="{{site.baseurl}}/en/humanoid_lab.html">EN</a>
	/
	<a href="{{site.baseurl}}/en/humanoid_lab_jp.html">JP</a>]
	  <br/>
	  <a href="https://www.tsukuba.ac.jp/en/academics/g-courses-cooperative/" target="blank_">University of Tsukuba - Cooperative Graduate School System <span class="glyphicon glyphicon-globe" style="font-size: small;"></span></a></small></h1>
    </div>
  </div>
</div>
</div>


<!-- Page Content -->
<div class="container">

  <table width="900" border="0" align="center" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
	<td style="padding: 20px; border-radius:15px">
      <table border="0" width="100%" cellpadding="7">
        <tbody>
          <tr>
            <td width="330" align="left" rowspan="2">
	      <img src="{{site.baseurl}}/en/assets/humanoidlab/four_humanoids.jpg" width="300" style="border-radius:15px">
	      <br>
	      <br>
	      <img src="{{site.baseurl}}/en/assets/humanoidlab/combined_logo.png" width="300" style="border-radius:15px">
	    </td>
            <td>
	      <p align="justify">
		The Humanoid Lab is a part of the CNRS-AIST JRL, located at AIST, Tsukuba about 5km from the main campus of the University of Tsukuba.
		It is associated to the university through the Cooperative Graduate School System, which means that graduate students at the university
		can work at JRL as Trainees and RAs under the supervision of Prof. Kanehiro (Faculty of Cooperative Graduate School, IMIS).
	      </p>
	      <p align="justify">
		The lab provides a unique opportunity for graduate students to work with Japanese and foreign research scientists on a
		wide variety of robot platforms and research topics. Our main research subjects include: task and motion planning and control, multimodal interaction
		with human and surrounding environment through perception, and cognitive robotics. </p>
	    </td>
	    <td valign="top" align="center" style="padding-left: 20px;padding-bottom: 20px" width="150" >
	      <a href="{{site.baseurl}}/en/members/member-kanehiro.html">
	      <img src="{{site.baseurl}}/en/assets/members/kanehiro.jpg" width="100" style="border-radius:50px">
	      </a>
	      <p align="center">KANEHIRO Fumio<br>
		<b>金広 文男</b></p>
	      <p style="margin-top: -0.5em; font-size:80%"><i>
	      f-kanehiro_*_aist.go.jp</i></p>
	      <p style="margin-top: -0.5em; background-color:#e0f0fe">Professor</p>
	    </td>
	  </tr>
	  <tr>
	    <td colspan="2">
	      <p align="justify">
		  Most members of our lab are bilingual (some are quadrilingual!),
		  hence, we encourage Japanese-speaking as well as English-speaking students to join our lab.
		  <b>The lab is always looking for talented and motivated graduate students to join our group. Students must be accepted to the Master's or
		  Doctoral Program in <a href="https://www.imis.tsukuba.ac.jp/admission">Intelligent and Mechanical Interaction Systems (IMIS)</a>,
		  University of Tsukuba through the regular admission procedure (examinations held in summer and winter).</b></p>

	      <p align="justify"><b>
		  If you're interested, please contact the lab or Prof. Kanehiro directly before you start the application procedure.</b></p>

	      <!-- <p>Check out our YouTube channel <a href="https://www.youtube.com/channel/UCYwHCdMHAKYZJ2MQIoTavVQ"> here. </a></p> -->
	      <p align="justify">(This page is maintained by current students.)</p>
	    </td
	  </tr>
        </tbody>
      </table>

      <div class="row">
	<h3 class="page-header">Research Content</h3>
	  <!-- content -->
	  <table border="0" width="100%" cellpadding="17" cellspacing="0">
            <tbody>
              <tr valign="top">
		<td style="padding: 20px;background-color:#e0f0fe; border-radius:15px">
		  <table border="0" width="100%" cellpadding="0" cellspacing="0" >
		    <tbody>
                      <tr>
			<td><b><font size="+1">Vision-based Belt Manipulation by Humanoid Robot</font></b></td>
			<td>&nbsp;</td>
			<td rowspan="3" valign="top" width="200">&nbsp;
			  <img src="{{site.baseurl}}/en/assets/humanoidlab/hl_03.jpg" height="227" border="0">
			</td>
                      </tr>
                      <tr>
			<td height="10"></td>
			<td height="10"></td>
                      </tr>
                      <tr>
			<td valign="top">Deformable objects are very
			common around us in our daily life. Because
			they have infinitely many degrees of freedom,
			they present a challenging problem in
			robotics. Inspired by practical industrial
			applications, we present our research on using
			a humanoid robot to take a long, thin and
			flexible belt out of a bobbin and pick up the
			bending part of the belt from the ground. By
			proposing a novel non-prehensile manipulation
			strategy “scraping” which utilizes the
			friction between the gripper and the surface
			of the belt, efficient manipulation can be
			achieved. In addition, a 3D shape detection
			algorithm for deformable objects is used
			during manipulation process. By integrating
			the novel “scraping” motion and the shape
			detection algorithm into our multi-objective
			QP-based controller, we show experimentally
			humanoid robots can complete this complex
			task.</td>
			<td width="50">&nbsp;</td>
                      </tr>
		    </tbody>
		  </table>
		</td>
              </tr>
            </tbody>
	  </table>

	  <br>
	  <br>

	  <!-- content -->
	  <table border="0" width="100%" cellpadding="17" cellspacing="0">
            <tbody>
              <tr valign="top">
		<td style="padding: 20px;background-color:#e0f0fe; border-radius:15px">
		  <table border="0" width="100%" cellpadding="0" cellspacing="0" >
		    <tbody>
                      <tr>
			<td><b><font size="+1">Simultaneous localization and mapping(SLAM) in dynamic environment</font></b></td>
			<td>&nbsp;</td>
			<td rowspan="3" valign="top" width="200">&nbsp;<img src="{{site.baseurl}}/en/assets/humanoidlab/hl_01_slam.jpg" height="227" border="0"></td>
                      </tr>
                      <tr>
			<td height="10"></td>
			<td height="10"></td>
                      </tr>
                      <tr>
			<td valign="top">Nowadays, SLAM in the dynamic environment has become a popular topic.
			  This problem is called dynamic SLAM where many solutions have been proposed to segment
			  out the dynamic objects that bring errors to camera tracking and subsequent 3D reconstruction.
			  However, state-of-the-art dynamic SLAM methods face the problems of accuracy and speed, which
			  is due to the fact that one segmentation algorithm cannot guarantee both points at the same time.
			  We propose a multi-purpose dynamic SLAM framework to provide a variety of selections for segmentation,
			  each has its applicable scene. Besides, if the user selects the semantic segmentation, the object-oriented
			  semantic mapping is beneficial for high level robotic tasks. </td>
			<td width="50">&nbsp;</td>
                      </tr>
		    </tbody>
		  </table>
		</td>
              </tr>
            </tbody>
	  </table>

	  <br>
	  <br>

	  <!-- content -->
	  <table border="0" width="100%" cellpadding="17" cellspacing="0">
            <tbody>
              <tr valign="top">
		<td style="padding: 20px;background-color:#e0f0fe; border-radius:15px">
		  <table border="0" width="100%" cellpadding="0" cellspacing="0">
		    <tbody>
                      <tr>
			<td><b><font size="+1">6-DoF Object Pose Estimation</font></b></td>
			<td>&nbsp;</td>
			<td rowspan="3" width="312"><img src="{{site.baseurl}}/en/assets/humanoidlab/hl_02.png" width="311" border="0"></td>
                      </tr>
                      <tr>
			<td height="10"></td>
			<td height="10"></td>
                      </tr>
                      <tr>
			<td valign="top">For a humanoid robot to interact with objects in its surrounding environment,
			  it is essential for the robot to find the position and orientation of the object relative to
			  itself - often through the use of its vision sensors. The 3D position and roll, pitch, yaw
			  rotation together comprise the 6 degrees-of-freedom pose of the object. For precise grasping
			  and manipulation of tools, this pose needs to be estimated with a  high degree of accuracy.
			  Further, we desire robustness against challenging lighting conditions, occlusions, and non-availability
			  of dense and accurate object models. This work mainly involves the use of Deep Learning based strategies
			  for solving problems in this sphere.
			</td>
			<td width="50">&nbsp;</td>
                      </tr>
		    </tbody>
		  </table>
		</td>
              </tr>
            </tbody>
	  </table>

	  <br>
	  <br>

	  <!-- content -->
	  <table border="0" width="100%" cellpadding="17" cellspacing="0">
            <tbody>
              <tr valign="top">
		<td style="padding: 20px;background-color:#e0f0fe; border-radius:15px">
		  <table border="0" width="100%" cellpadding="0" cellspacing="0">
		    <tbody>
                      <tr>
			<td><b><font size="+1">Enhanced Visual Feedback with Decoupled Viewpoint Control in Immersive Teleoperation using SLAM</font></b></td>
			<td>&nbsp;</td>
			<td rowspan="3" width="312"><img src="{{site.baseurl}}/en/assets/humanoidlab/hl_cslam.png" width="311"  border="0"></td>
                      </tr>
                      <tr>
			<td height="10"></td>
			<td height="10"></td>
                      </tr>
                      <tr>
			<td valign="top">During humanoid robot teleoperation, there is a noticeable delay between the motion of the
				operator’s and robot’s head. This latency could cause the lag in visual feedback, which decreases the
				immersion of the system, may cause some dizziness and reduce the efficiency of interaction in teleoperation
				since operator needs to wait for the real-time visual feedback. To solve this problem, we
				developed a decoupled viewpoint control solution which allows the operator to
				obtain the visual feedback changes with low-latency in VR and to increase the reachable
				visibility range. Besides, we propose a complementary SLAM solution which uses the reconstructed mesh to
				complement the blank area that is not covered by the real-time robot’s point cloud visual feedback. The
				operator could sense the robot head’s real-time orientation by observing the pose of the point cloud.
			</td>
			<td width="50">&nbsp;</td>
                      </tr>
		    </tbody>
		  </table>
		</td>
              </tr>
            </tbody>
	  </table>

	  <br>
	  <br>

	  <!-- content -->
	  <table border="0" width="100%" cellpadding="17" cellspacing="0">
            <tbody>
              <tr valign="top">
		<td style="padding: 20px;background-color:#e0f0fe; border-radius:15px">
		  <table border="0" width="100%" cellpadding="0" cellspacing="0">
		    <tbody>
                      <tr>
			<td><b><font size="+1">Bipedal Walking With Footstep Plans via Reinforcement Learning</font></b></td>
			<td>&nbsp;</td>
			<td rowspan="3" width="312">
			  <iframe width="311" src="https://www.youtube.com/embed/-mxaQ-f9Ee4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			</td>
                      </tr>
                      <tr>
			<td height="10"></td>
			<td height="10"></td>
                      </tr>
                      <tr>
			<td valign="top">
                         To enable application of RL policy controller humanoid robots
                         in real-world settings, it is crucial to build a system that can
                         achieve robust walking in any direction, on 2D and 3D terrains,
                         and be controllable by a user-command. In this paper, we
                         tackle this problem by learning a policy to follow a given
                         step sequence. The policy is trained with the help of a set
			 of procedurally generated step sequences (also called footstep
                         plans). We show that simply feeding the upcoming 2 steps to the
                         policy is sufficient to achieve omindirectional walking, turning
                         in place, standing, and climbing stairs. Our method employs
                         curriculum learning on the objective function and on sample
                         complexity, and circumvents the need for reference motions
                         or pre-trained weights. We demonstrate the application of our
                         proposed method to learn RL policies for 3 notably distinct
                         robot platforms - HRP5P, JVRC-1, and Cassie, in the MuJoCo
			</td>
			<td width="50">&nbsp;</td>
                      </tr>
		    </tbody>
		  </table>
		</td>
              </tr>
            </tbody>
	  </table>

      </div>

      <div class="row">
	<h3 class="page-header">Student Members</h3>
        <table width="100%" border="0" cellspacing="0" class="table table-striped">
          <tbody>
            <tr>
	      <th width="25%">Name</th>
	      <th width="25%">Grade</th>
	      <th width="25%">Email <br>
		(replace the _*_ with @)</th>
            </tr>
	    <!--
		<tr>
		  <td><a href="{{site.baseurl}}/en/members/member-kanehiro.html">金広 文男</a></td>
		  <td>連携大学院教授</td>
		  <td width="25%">f-kanehiro_*_aist.go.jp</td>
		</tr>
		-->
	    <tr>
	      <td><a href="{{site.baseurl}}/en/members/member-qin.html">Yili Qin</a></td>
	      <td>Ph.D. 4th Year</td>
	      <td width="25%">yili.tan_*_aist.go.jp</td>
            </tr>
            <tr>
	      <td><a href="{{site.baseurl}}/en/members/member-sun.html">Leyuan Sun</a></td>
	      <td>Ph.D. 3rd Year</td>
	      <td width="25%">son.leyuansun_*_aist.go.jp</td>
            </tr>
            <tr>
	      <td><a href="{{site.baseurl}}/en/members/member-singh.html">Rohan Pratap Singh</a></td>
	      <td>Ph.D. 2nd Year</td>
	      <td width="25%">rohan-singh_*_aist.go.jp</td>
            </tr>
            <tr>
	      <td>Xinchi Gao</td>
	      <td>Master's 1st Year</td>
	      <td width="25%"></td>
            </tr>
            <tr>
	      <td>Okumiya Tomoya</td>
	      <td>Master's 1st Year</td>
	      <td width="25%"></td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="row">
	<div class="col-lg-12">
	  <h3 class="page-header">Location</h3>
          <table width="100%">
            <tbody>
              <tr align="center">
		<td>
		  <iframe src="https://www.google.com/maps/embed?pb=!1m28!1m12!1m3!1d25795.692830871016!2d140.1074103178154!3d36.082234076560354!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!4m13!3e6!4m5!1s0x60220bff99f57b0b%3A0x1cad40e7632fb4b8!2zVW5pdmVyc2l0eSBvZiBUc3VrdWJhIOetkeazouWkpw!3m2!1d36.103866599999996!2d140.1020979!4m5!1s0x60220cc567b824f5%3A0xecc14922713a4044!2z44CSMzA1LTg1NjAgSWJhcmFraSwgVHN1a3ViYSwgVW1lem9ubywgMSBDaG9tZeKIkjEtMSDkuK3lpK7nrKwx44Gk44GP44Gw5pys6YOo5oOF5aCx5oqA6KGT5YWx5ZCM56CU56m25qOfIFRzdWt1YmEgQ2VudGVyLCBBSVNUOiBOYXRpb25hbCBJbnN0aXR1dGUgb2YgQWR2YW5jZWQgSW5kdXN0cmlhbCBTY2llbmNlIGFuZCBUZWNobm9sb2d5!3m2!1d36.0624307!2d140.1356783!5e0!3m2!1sen!2sjp!4v1650429789876!5m2!1sen!2sjp" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
		</td>
              </tr>
            </tbody>
          </table>
	</div>
      </div>

      </td></tr>
    <tbody>
  </table>
</div>
<!-- /.container -->

